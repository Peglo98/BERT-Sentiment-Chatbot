# BERT Sentiment Chatbot

A complete pipeline for training, evaluating and deploying a BERT-based sentiment analysis chatbot. It converts raw Amazon product reviews into a binary sentiment dataset (positive/negative), trains a `bert-base-uncased` classifier, evaluates its performance, and exposes a simple Flask web app for real-time inference.

## Table of Contents

- [Features](#features)  
- [Project Structure](#project-structure)  
- [Requirements](#requirements)  
- [Data Preparation](#data-preparation)  
- [Training the Model](#training-the-model)  
- [Evaluating the Model](#evaluating-the-model)  
- [Running the Web App](#running-the-web-app)  
- [File Descriptions](#file-descriptions)  
- [References](#references)  

## Features

- **Data conversion**: simplify raw Amazon reviews to JSON lines with `text` and binary `label` (`1` if rating ≥ 3, else `0`).  
- **Model training**: fine-tune a BERT classifier with PyTorch and the Hugging Face Transformers library.  
- **Evaluation**: batch-wise and final accuracy reporting on held-out or custom test sets.  
- **Web interface**: Flask app with a chat-style HTML page for submitting reviews and receiving sentiment predictions.

## Project Structure

```
.
├── aplikacja.py
├── konwersja.py
├── trenowanie.py
├── test.py
├── reviews.json                # Raw dataset (download separately)
├── simplified_reviews.json     # Generated by konwersja.py
├── bert_sentiment_model.pth    # Saved model weights
└── templates/
    └── chat.html               # Flask front-end template
```

## Requirements

- Python 3.7+  
- torch  
- transformers  
- flask  
- scikit-learn  
- tqdm  

Install with:

```bash
pip install torch transformers flask scikit-learn tqdm
```

## Data Preparation

1. **Download** the raw review dataset (e.g. from Hugging Face’s Amazon Reviews 2023).  
2. **Place** `reviews.json` in the project root; it must be a JSON-lines file where each line has at least:
   ```json
   {
     "text": "Review text …",
     "rating": 1  // integer from 1 to 5
   }
   ```
3. **Run** the conversion script to produce a simplified binary dataset:
   ```bash
   python konwersja.py
   ```
   This writes `simplified_reviews.json` with lines of the form:
   ```json
   { "text": "...", "label": 1 }
   ```

## Training the Model

Train (or reload) the BERT classifier:

```bash
python trenowanie.py
```

- Splits `simplified_reviews.json` into a 90% train / 10% test split  
- Fine-tunes for 3 epochs by default  
- Saves the model weights to `bert_sentiment_model.pth`

## Evaluating the Model

Evaluate on a separate simplified dataset:

```bash
python test.py
```

- Expects `output.json` (same format as `simplified_reviews.json`)  
- Reports batch-level accuracy during evaluation and final test accuracy

## Running the Web App

Start the Flask application:

```bash
python aplikacja.py
```

Then open your browser at `http://127.0.0.1:5000/` to submit reviews and view live sentiment predictions.

## File Descriptions

- **aplikacja.py**  
  Loads the trained model and tokenizer, serves two endpoints:  
  - `/` renders `chat.html`  
  - `/predict` accepts JSON `{ "review": "…" }` and returns `{ "sentiment": "positive" | "negative" }`

- **konwersja.py**  
  Reads `reviews.json`, applies `rating_to_label(r) → 1 if r ≥ 3 else 0`, writes `simplified_reviews.json`.

- **trenowanie.py**  
  Defines `ReviewDataset`, training, evaluation, save/load functions, and an interactive CLI loop for ad-hoc sentiment queries.

- **test.py**  
  Loads the saved model, builds a `DataLoader` for a custom test set (`output.json`), and prints evaluation metrics.

- **templates/chat.html**  
  Simple HTML/JavaScript UI to send review text to the Flask backend and display the result.

## References

- McAuley-Lab, “Amazon Reviews 2023” dataset on Hugging Face:  
  https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023  
- Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”  
- Wolf et al., “Transformers: State-of-the-art Natural Language Processing” (Hugging Face Transformers library)  
- Kingma & Ba, “Adam: A Method for Stochastic Optimization”
